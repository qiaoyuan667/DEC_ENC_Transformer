{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from jaxtyping import Float, Int\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class AttentionBias(nn.Module, abc.ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        num_groups: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert num_heads > 0 and dim % num_heads == 0\n",
    "        assert (num_heads % num_groups == 0) and (num_heads >= num_groups)\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.num_groups = num_groups\n",
    "        self.heads_per_group = num_heads // num_groups\n",
    "        self.head_dim = dim // num_heads\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def forward(\n",
    "        self,\n",
    "        query: Float[torch.Tensor, \"*batch group hpg q_len dim\"],\n",
    "        key: Float[torch.Tensor, \"*batch group hpg kv_len dim\"],\n",
    "        query_id: Int[torch.Tensor, \"*batch 1 1 q_len\"],\n",
    "        kv_id: Int[torch.Tensor, \"*batch 1 1 kv_len\"],\n",
    "    ) -> Float[torch.Tensor, \"*batch #group #hpg q_len kv_len\"]: ...\n",
    "\n",
    "\n",
    "class RelativeAttentionBias(AttentionBias):\n",
    "    def __init__(self, num_buckets: int, dim: int, num_heads: int, num_groups: int):\n",
    "        super().__init__(dim, num_heads, num_groups)\n",
    "        self.emb = nn.Embedding(\n",
    "            num_embeddings=num_buckets, embedding_dim=self.num_heads\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: Float[torch.Tensor, \"*batch group hpg q_len dim\"],\n",
    "        key: Float[torch.Tensor, \"*batch group hpg kv_len dim\"],\n",
    "        query_id: Int[torch.Tensor, \"*batch 1 1 q_len\"],\n",
    "        kv_id: Int[torch.Tensor, \"*batch 1 1 kv_len\"],\n",
    "    ) -> Float[torch.Tensor, \"*batch #group #hpg q_len kv_len\"]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class BinaryAttentionBias(AttentionBias):\n",
    "    def __init__(self, dim: int, num_heads: int, num_groups: int):\n",
    "        super().__init__(dim, num_heads, num_groups)\n",
    "        self.emb = nn.Embedding(num_embeddings=2, embedding_dim=self.num_heads)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: Float[torch.Tensor, \"*batch group hpg q_len dim\"],\n",
    "        key: Float[torch.Tensor, \"*batch group hpg kv_len dim\"],\n",
    "        query_id: Int[torch.Tensor, \"*batch 1 1 q_len\"],\n",
    "        kv_id: Int[torch.Tensor, \"*batch 1 1 kv_len\"],\n",
    "    ) -> Float[torch.Tensor, \"*batch #group #hpg q_len kv_len\"]:\n",
    "        ind = torch.eq(query_id.unsqueeze(-1), kv_id.unsqueeze(-2))\n",
    "        weight = rearrange(self.emb.weight, \"two num_heads -> two num_heads 1 1\")\n",
    "        bias = rearrange(  # try to avoid advanced indexing\n",
    "            ~ind * weight[:1] + ind * weight[1:],\n",
    "            \"... 1 (group hpg) q_len kv_len -> ... group hpg q_len kv_len\",\n",
    "            group=self.num_groups,\n",
    "            hpg=self.heads_per_group,\n",
    "        )\n",
    "        return bias\n",
    "\n",
    "\n",
    "class LinearAttentionBias(AttentionBias):\n",
    "    def __init__(self, dim: int, num_heads: int, num_groups: int):\n",
    "        super().__init__(dim, num_heads, num_groups)\n",
    "        m = 0.5 ** ((1 + torch.arange(self.num_heads)) * (8 / self.num_heads))\n",
    "        m = rearrange(\n",
    "            m,\n",
    "            \"(group hpg) -> group hpg 1 1\",\n",
    "            group=self.num_groups,\n",
    "            hpg=self.heads_per_group,\n",
    "        )\n",
    "        self.register_buffer(\"m\", m)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: Float[torch.Tensor, \"*batch group hpg q_len dim\"],\n",
    "        key: Float[torch.Tensor, \"*batch group hpg kv_len dim\"],\n",
    "        query_id: Int[torch.Tensor, \"*batch 1 1 q_len\"],\n",
    "        kv_id: Int[torch.Tensor, \"*batch 1 1 kv_len\"],\n",
    "    ) -> Float[torch.Tensor, \"*batch #group #hpg q_len kv_len\"]:\n",
    "        ind = kv_id.unsqueeze(-2) - query_id.unsqueeze(-1)\n",
    "        return self.m * ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import math\n",
    "from functools import cached_property\n",
    "from typing import Any, Optional\n",
    "\n",
    "import torch\n",
    "from einops import einsum, rearrange, repeat\n",
    "from jaxtyping import Float, Int\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Projection(nn.Module, abc.ABC):\n",
    "    def __init__(self, proj_width: int, num_heads: int, num_groups: int, **kwargs: Any):\n",
    "        super().__init__()\n",
    "        self.proj_width = proj_width\n",
    "        self.num_heads = num_heads\n",
    "        self.num_groups = num_groups\n",
    "        self.heads_per_group = num_heads // num_groups\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Float[torch.Tensor, \"*batch group hpg seq dim\"],\n",
    "        seq_id: Optional[Int[torch.Tensor, \"*batch #group #hpg seq\"]],\n",
    "    ) -> Float[torch.Tensor, \"*batch group hpg seq dim\"]: ...\n",
    "\n",
    "\n",
    "class IdentityProjection(Projection):\n",
    "    def __init__(self, *, proj_width: int, num_heads: int, num_groups: int, **kwargs):\n",
    "        super().__init__(proj_width, num_heads, num_groups)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Float[torch.Tensor, \"*batch group hpg seq dim\"],\n",
    "        seq_id: Optional[Int[torch.Tensor, \"*batch #group #hpg seq\"]] = None,\n",
    "    ) -> Float[torch.Tensor, \"*batch group hpg seq dim\"]:\n",
    "        return x\n",
    "\n",
    "\n",
    "class RotaryProjection(Projection):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        proj_width: int,\n",
    "        num_heads: int,\n",
    "        num_groups: int,\n",
    "        max_len: int = 512,\n",
    "        base: int = 10000,\n",
    "    ):\n",
    "        super().__init__(proj_width, num_heads, num_groups)\n",
    "        assert (\n",
    "            self.proj_width % 2 == 0\n",
    "        ), f\"proj_width must be even, got {self.proj_width}\"\n",
    "        self.register_buffer(\n",
    "            \"theta\",\n",
    "            1.0\n",
    "            / torch.pow(\n",
    "                base,\n",
    "                torch.arange(0, self.proj_width, 2, dtype=torch.float)\n",
    "                / self.proj_width,\n",
    "            ),\n",
    "            persistent=False,\n",
    "        )\n",
    "        self.register_buffer(\"cos\", None, persistent=False)\n",
    "        self.register_buffer(\"sin\", None, persistent=False)\n",
    "        self._init_freq(max_len=max_len)\n",
    "\n",
    "    def _init_freq(self, max_len: int):\n",
    "        if self.cos is None or self.cos.size(-2) < max_len:\n",
    "            position = torch.arange(\n",
    "                max_len, device=self.theta.device, dtype=self.theta.dtype\n",
    "            )\n",
    "            m_theta = einsum(position, self.theta, \"length, width -> length width\")\n",
    "            m_theta = repeat(m_theta, \"length width -> length (width 2)\")\n",
    "            self.register_buffer(\"cos\", torch.cos(m_theta), persistent=False)\n",
    "            self.register_buffer(\"sin\", torch.sin(m_theta), persistent=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def _rotate(x: Float[torch.Tensor, \"... dim\"]) -> Float[torch.Tensor, \"... dim\"]:\n",
    "        x1, x2 = rearrange(x, \"... (dim r) -> r ... dim\", r=2)\n",
    "        return rearrange([-x2, x1], \"r ... dim -> ... (dim r)\", r=2)  # noqa\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Float[torch.Tensor, \"*batch group hpg seq dim\"],\n",
    "        seq_id: Optional[Int[torch.Tensor, \"*batch #group #hpg seq\"]],\n",
    "    ) -> Float[torch.Tensor, \"*batch group hpg seq dim\"]:\n",
    "        self._init_freq(max_len=seq_id.max() + 1)\n",
    "        rot_cos = self.cos[seq_id]\n",
    "        rot_sin = self.sin[seq_id]\n",
    "        return rot_cos * x + rot_sin * self._rotate(x)\n",
    "\n",
    "\n",
    "class LearnedProjection(Projection):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        proj_width: int,\n",
    "        num_heads: int,\n",
    "        num_groups: int,\n",
    "        max_len: int = 512,\n",
    "    ):\n",
    "        super().__init__(proj_width, num_heads, num_groups)\n",
    "        self.max_len = max_len\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.empty((max_len, self.proj_width, self.proj_width))\n",
    "        )\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for idx in range(self.max_len):\n",
    "            nn.init.kaiming_uniform_(self.weight[idx], a=math.sqrt(5))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Float[torch.Tensor, \"*batch group hpg seq dim\"],\n",
    "        seq_id: Optional[Int[torch.Tensor, \"*batch #group #hpg seq\"]],\n",
    "    ) -> Float[torch.Tensor, \"*batch group hpg seq dim\"]:\n",
    "        weight = self.weight[seq_id]\n",
    "        return einsum(weight, x, \"... out inp, ... inp -> ... out\")\n",
    "\n",
    "\n",
    "class QueryKeyProjection(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        num_groups: int,\n",
    "        proj_layer: type[Projection],\n",
    "        kwargs: Optional[dict[str, Any]] = None,\n",
    "        key_proj_layer: Optional[type[Projection]] = None,\n",
    "        key_kwargs: Optional[dict[str, Any]] = None,\n",
    "        partial_factor: Optional[tuple[float, float]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if partial_factor is not None:\n",
    "            assert (\n",
    "                0.0 <= partial_factor[0] < partial_factor[1] <= 1.0\n",
    "            ), f\"got {partial_factor[0]}, {partial_factor[1]}\"\n",
    "        assert num_heads > 0 and dim % num_heads == 0\n",
    "        assert (num_heads % num_groups == 0) and (num_heads >= num_groups)\n",
    "\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.partial_factor = partial_factor\n",
    "        self.query_proj = proj_layer(\n",
    "            proj_width=self.proj_width,\n",
    "            num_heads=num_heads,\n",
    "            num_groups=num_groups,\n",
    "            **(kwargs or {}),\n",
    "        )\n",
    "        if key_proj_layer is None:\n",
    "            self.key_proj = self.query_proj\n",
    "        else:\n",
    "            self.key_proj = key_proj_layer(\n",
    "                proj_width=self.proj_width,\n",
    "                num_heads=num_heads,\n",
    "                num_groups=num_groups,\n",
    "                **(key_kwargs or {}),\n",
    "            )\n",
    "\n",
    "    @cached_property\n",
    "    def proj_width(self) -> int:\n",
    "        if self.partial_factor is None:\n",
    "            return self.head_dim\n",
    "        return int(self.head_dim * (self.partial_factor[1] - self.partial_factor[0]))\n",
    "\n",
    "    @cached_property\n",
    "    def split_sizes(self) -> tuple[int, int, int]:\n",
    "        if self.partial_factor is None:\n",
    "            return 0, self.head_dim, 0\n",
    "        return (\n",
    "            int(self.partial_factor[0] * self.head_dim),\n",
    "            self.proj_width,\n",
    "            int((1.0 - self.partial_factor[1]) * self.head_dim),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: Float[torch.Tensor, \"*batch group hpg q_len dim\"],\n",
    "        key: Float[torch.Tensor, \"*batch group hpg kv_len dim\"],\n",
    "        query_id: Optional[Int[torch.Tensor, \"*batch #group #hpg q_len\"]],\n",
    "        kv_id: Optional[Int[torch.Tensor, \"*batch #group #hpg kv_len\"]],\n",
    "    ) -> tuple[\n",
    "        Float[torch.Tensor, \"*batch group hpg seq dim\"],\n",
    "        Float[torch.Tensor, \"*batch group hpg seq dim\"],\n",
    "    ]:\n",
    "        if self.partial_factor is not None:\n",
    "            queries = list(query.split(self.split_sizes, dim=-1))\n",
    "            keys = list(key.split(self.split_sizes, dim=-1))\n",
    "            queries[1] = self.query_proj(queries[1], seq_id=query_id)\n",
    "            keys[1] = self.key_proj(keys[1], seq_id=kv_id)\n",
    "            query = torch.cat(queries, dim=-1)\n",
    "            key = torch.cat(keys, dim=-1)\n",
    "        else:\n",
    "            query = self.query_proj(query, seq_id=query_id)\n",
    "            key = self.key_proj(key, seq_id=kv_id)\n",
    "        return query, key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections.abc import Callable\n",
    "from functools import partial\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from jaxtyping import Bool, Float, Int\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Support returning weights\n",
    "# TODO: Support caching (return past_key_value)\n",
    "\n",
    "\n",
    "def native_scaled_dot_product_attention(\n",
    "    query: Float[torch.Tensor, \"*batch group hpg q_len dim\"],\n",
    "    key: Float[torch.Tensor, \"*batch group hpg kv_len dim\"],\n",
    "    value: Float[torch.Tensor, \"*batch group hpg kv_len dim\"],\n",
    "    attn_mask: Optional[\n",
    "        Bool[torch.Tensor, \"*batch #group #hpg q_len kv_len\"]\n",
    "        | Float[torch.Tensor, \"*batch #group #hpg q_len kv_len\"]\n",
    "    ] = None,\n",
    "    dropout_p: float = 0.0,\n",
    "    scale: Optional[float] = None,\n",
    "):\n",
    "    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n",
    "    attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            attn_bias = torch.zeros_like(attn_weight)\n",
    "            attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
    "        else:\n",
    "            attn_bias = attn_mask\n",
    "        attn_weight = attn_weight + attn_bias\n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "    attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "    return attn_weight @ value\n",
    "\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        num_groups: int,\n",
    "        bias: bool = True,\n",
    "        norm_layer: Optional[type[nn.Module] | partial[nn.Module]] = nn.LayerNorm,\n",
    "        softmax_scale: Optional[float] = None,\n",
    "        attn_dropout_p: float = 0.0,\n",
    "        var_attn_bias: Optional[Callable[[], AttentionBias]] = None,\n",
    "        time_attn_bias: Optional[Callable[[], AttentionBias]] = None,\n",
    "        var_qk_proj: Optional[Callable[[], QueryKeyProjection]] = None,\n",
    "        time_qk_proj: Optional[Callable[[], QueryKeyProjection]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert num_heads > 0 and dim % num_heads == 0\n",
    "        assert (num_heads % num_groups == 0) and (num_heads >= num_groups)\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.num_groups = num_groups\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.heads_per_group = num_heads // num_groups\n",
    "        self.var_attn_bias = var_attn_bias() if var_attn_bias is not None else None\n",
    "        self.time_attn_bias = time_attn_bias() if time_attn_bias is not None else None\n",
    "        self.var_qk_proj = var_qk_proj() if var_qk_proj is not None else None\n",
    "        self.time_qk_proj = time_qk_proj() if time_qk_proj is not None else None\n",
    "\n",
    "        self.softmax_scale = softmax_scale or 1 / math.sqrt(self.head_dim)\n",
    "\n",
    "        self.q_proj = nn.Linear(dim, dim, bias=bias)\n",
    "        self.k_proj = nn.Linear(dim, self.head_dim * num_groups, bias=bias)\n",
    "        self.v_proj = nn.Linear(dim, self.head_dim * num_groups, bias=bias)\n",
    "        self.q_norm = (\n",
    "            norm_layer(self.head_dim) if norm_layer is not None else nn.Identity()\n",
    "        )\n",
    "        self.k_norm = (\n",
    "            norm_layer(self.head_dim) if norm_layer is not None else nn.Identity()\n",
    "        )\n",
    "        self.attn_dropout_p = attn_dropout_p\n",
    "        self.out_proj = nn.Linear(dim, dim, bias=bias)\n",
    "\n",
    "    def _get_var_id(\n",
    "        self,\n",
    "        query: Float[torch.Tensor, \"*batch group hpg q_len dim\"],\n",
    "        key: Float[torch.Tensor, \"*batch group hpg kv_len dim\"],\n",
    "        query_var_id: Optional[Int[torch.Tensor, \"*batch q_len\"]],\n",
    "        kv_var_id: Optional[Int[torch.Tensor, \"*batch kv_len\"]],\n",
    "    ) -> tuple[\n",
    "        Optional[Int[torch.Tensor, \"*batch #group #hpg q_len\"]],\n",
    "        Optional[Int[torch.Tensor, \"*batch #group #hpg kv_len\"]],\n",
    "    ]:\n",
    "        if self.var_attn_bias is not None or self.var_qk_proj is not None:\n",
    "            if query_var_id is None:\n",
    "                query_var_id = repeat(\n",
    "                    torch.zeros((), device=query.device, dtype=torch.long),\n",
    "                    f\" -> {' '.join(map(str, query.shape[:-4]))} 1 1 {query.shape[-2]}\",\n",
    "                )\n",
    "            else:\n",
    "                query_var_id = rearrange(query_var_id, \"... q_len -> ... 1 1 q_len\")\n",
    "\n",
    "            if kv_var_id is None:\n",
    "                kv_var_id = repeat(\n",
    "                    torch.zeros((), device=key.device, dtype=torch.long),\n",
    "                    f\" -> {' '.join(map(str, key.shape[:-4]))} 1 1 {key.shape[-2]}\",\n",
    "                )\n",
    "            else:\n",
    "                kv_var_id = rearrange(kv_var_id, \"... kv_len -> ... 1 1 kv_len\")\n",
    "\n",
    "        return query_var_id, kv_var_id\n",
    "\n",
    "    def _get_time_id(\n",
    "        self,\n",
    "        query: Float[torch.Tensor, \"*batch group hpg q_len dim\"],\n",
    "        key: Float[torch.Tensor, \"*batch group hpg kv_len dim\"],\n",
    "        query_time_id: Optional[Int[torch.Tensor, \"*batch q_len\"]],\n",
    "        kv_time_id: Optional[Int[torch.Tensor, \"*batch kv_len\"]],\n",
    "    ) -> tuple[\n",
    "        Optional[Int[torch.Tensor, \"*batch 1 1 q_len\"]],\n",
    "        Optional[Int[torch.Tensor, \"*batch 1 1 kv_len\"]],\n",
    "    ]:\n",
    "        if self.time_attn_bias is not None or self.time_qk_proj is not None:\n",
    "            if query_time_id is None:\n",
    "                query_time_id = repeat(\n",
    "                    torch.arange(\n",
    "                        query.shape[-2], device=query.device, dtype=torch.long\n",
    "                    ),\n",
    "                    f\"q_len -> {' '.join(map(str, query.shape[:-4]))} 1 1 q_len\",\n",
    "                )\n",
    "            else:\n",
    "                query_time_id = rearrange(query_time_id, \"... q_len -> ... 1 1 q_len\")\n",
    "\n",
    "            if kv_time_id is None:\n",
    "                kv_time_id = repeat(\n",
    "                    torch.arange(key.shape[-2], device=key.device, dtype=torch.long),\n",
    "                    f\"kv_len -> {' '.join(map(str, key.shape[:-4]))} 1 1 kv_len\",\n",
    "                )\n",
    "            else:\n",
    "                kv_time_id = rearrange(kv_time_id, \"... kv_len-> ... 1 1 kv_len\")\n",
    "\n",
    "        return query_time_id, kv_time_id\n",
    "\n",
    "    def _update_attn_mask(\n",
    "        self,\n",
    "        attn_mask: Optional[Bool[torch.Tensor, \"*batch q_len kv_len\"]],\n",
    "        query: Float[torch.Tensor, \"*batch group hpg q_len dim\"],\n",
    "        key: Float[torch.Tensor, \"*batch group hpg kv_len dim\"],\n",
    "        query_var_id: Optional[Int[torch.Tensor, \"*batch 1 1 q_len\"]] = None,\n",
    "        kv_var_id: Optional[Int[torch.Tensor, \"*batch 1 1 kv_len\"]] = None,\n",
    "        query_time_id: Optional[Int[torch.Tensor, \"*batch 1 1 q_len\"]] = None,\n",
    "        kv_time_id: Optional[Int[torch.Tensor, \"*batch 1 1 kv_len\"]] = None,\n",
    "    ) -> Optional[\n",
    "        Bool[torch.Tensor, \"*batch #group #hpg q_len kv_len\"]\n",
    "        | Float[torch.Tensor, \"*batch #group #hpg q_len kv_len\"]\n",
    "    ]:\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = rearrange(\n",
    "                attn_mask,\n",
    "                \"... q_len kv_len -> ... 1 1 q_len kv_len\",\n",
    "            )\n",
    "\n",
    "        attn_bias = 0\n",
    "        if self.var_attn_bias is not None:\n",
    "            attn_bias = attn_bias + self.var_attn_bias(\n",
    "                query,\n",
    "                key,\n",
    "                query_id=query_var_id,\n",
    "                kv_id=kv_var_id,\n",
    "            )\n",
    "\n",
    "        if self.time_attn_bias is not None:\n",
    "            attn_bias = attn_bias + self.time_attn_bias(\n",
    "                query,\n",
    "                key,\n",
    "                query_id=query_time_id,\n",
    "                kv_id=kv_time_id,\n",
    "            )\n",
    "\n",
    "        attn_mask = (\n",
    "            attn_mask\n",
    "            if isinstance(attn_bias, int)\n",
    "            else (\n",
    "                attn_bias\n",
    "                if attn_mask is None\n",
    "                else attn_bias.masked_fill(attn_mask.logical_not(), float(\"-inf\"))\n",
    "            )\n",
    "        )\n",
    "        return attn_mask\n",
    "\n",
    "    def _qk_proj(\n",
    "        self,\n",
    "        query: Float[torch.Tensor, \"*batch group hpg q_len dim\"],\n",
    "        key: Float[torch.Tensor, \"*batch group hpg kv_len dim\"],\n",
    "        query_var_id: Optional[Int[torch.Tensor, \"*batch #group #hpg q_len\"]],\n",
    "        kv_var_id: Optional[Int[torch.Tensor, \"*batch #group #hpg kv_len\"]],\n",
    "        query_time_id: Optional[Int[torch.Tensor, \"*batch #group #hpg q_len\"]],\n",
    "        kv_time_id: Optional[Int[torch.Tensor, \"*batch #group #hpg kv_len\"]],\n",
    "    ) -> tuple[\n",
    "        Float[torch.Tensor, \"*batch group hpg q_len dim\"],\n",
    "        Float[torch.Tensor, \"*batch group hpg kv_len dim\"],\n",
    "    ]:\n",
    "        if self.var_qk_proj is not None:\n",
    "            query, key = self.var_qk_proj(\n",
    "                query, key, query_id=query_var_id, kv_id=kv_var_id\n",
    "            )\n",
    "\n",
    "        if self.time_qk_proj is not None:\n",
    "            query, key = self.time_qk_proj(\n",
    "                query, key, query_id=query_time_id, kv_id=kv_time_id\n",
    "            )\n",
    "\n",
    "        return query, key\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: Float[torch.Tensor, \"*batch q_len dim\"],\n",
    "        key: Float[torch.Tensor, \"*batch kv_len dim\"],\n",
    "        value: Float[torch.Tensor, \"*batch kv_len dim\"],\n",
    "        attn_mask: Optional[Bool[torch.Tensor, \"*batch q_len kv_len\"]] = None,\n",
    "        query_var_id: Optional[Int[torch.Tensor, \"*batch q_len\"]] = None,\n",
    "        kv_var_id: Optional[Int[torch.Tensor, \"*batch kv_len\"]] = None,\n",
    "        query_time_id: Optional[Int[torch.Tensor, \"*batch q_len\"]] = None,\n",
    "        kv_time_id: Optional[Int[torch.Tensor, \"*batch kv_len\"]] = None,\n",
    "    ) -> Float[torch.Tensor, \"*batch q_len dim\"]:\n",
    "        query = self.q_proj(query)\n",
    "        key = self.k_proj(key)\n",
    "        value = self.v_proj(value)\n",
    "\n",
    "        query = self.q_norm(\n",
    "            rearrange(\n",
    "                query,\n",
    "                \"... q_len (group hpg dim) -> ... group hpg q_len dim\",\n",
    "                group=self.num_groups,\n",
    "                hpg=self.heads_per_group,\n",
    "            )\n",
    "        )\n",
    "        key = self.k_norm(\n",
    "            repeat(\n",
    "                key,\n",
    "                \"... kv_len (group dim) -> ... group hpg kv_len dim\",\n",
    "                group=self.num_groups,\n",
    "                hpg=self.heads_per_group,\n",
    "            )\n",
    "        )\n",
    "        value = repeat(\n",
    "            value,\n",
    "            \"... kv_len (group dim) -> ... group hpg kv_len dim\",\n",
    "            group=self.num_groups,\n",
    "            hpg=self.heads_per_group,\n",
    "        )\n",
    "\n",
    "        query_var_id, kv_var_id = self._get_var_id(query, key, query_var_id, kv_var_id)\n",
    "        query_time_id, kv_time_id = self._get_time_id(\n",
    "            query,\n",
    "            key,\n",
    "            query_time_id,\n",
    "            kv_time_id,\n",
    "        )\n",
    "\n",
    "        attn_mask = self._update_attn_mask(\n",
    "            attn_mask,\n",
    "            query,\n",
    "            key,\n",
    "            query_var_id=query_var_id,\n",
    "            kv_var_id=kv_var_id,\n",
    "            query_time_id=query_time_id,\n",
    "            kv_time_id=kv_time_id,\n",
    "        )\n",
    "\n",
    "        query, key = self._qk_proj(\n",
    "            query,\n",
    "            key,\n",
    "            query_var_id=query_var_id,\n",
    "            kv_var_id=kv_var_id,\n",
    "            query_time_id=query_time_id,\n",
    "            kv_time_id=kv_time_id,\n",
    "        )\n",
    "\n",
    "        out = F.scaled_dot_product_attention(\n",
    "            query,\n",
    "            key,\n",
    "            value,\n",
    "            attn_mask=attn_mask,\n",
    "            dropout_p=self.attn_dropout_p,\n",
    "            scale=self.softmax_scale,\n",
    "        )\n",
    "        out = rearrange(out, \"... group hpg q_len dim -> ... q_len (group hpg dim)\")\n",
    "        return self.out_proj(out)\n",
    "\n",
    "\n",
    "class MultiQueryAttention(GroupedQueryAttention):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        bias: bool = True,\n",
    "        norm_layer: Optional[type[nn.Module] | partial[nn.Module]] = nn.LayerNorm,\n",
    "        softmax_scale: Optional[float] = None,\n",
    "        attn_dropout_p: float = 0.0,\n",
    "        var_attn_bias: Optional[Callable[[], AttentionBias]] = None,\n",
    "        time_attn_bias: Optional[Callable[[], AttentionBias]] = None,\n",
    "        var_qk_proj: Optional[Callable[[], QueryKeyProjection]] = None,\n",
    "        time_qk_proj: Optional[Callable[[], QueryKeyProjection]] = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            dim=dim,\n",
    "            num_heads=num_heads,\n",
    "            num_groups=1,\n",
    "            bias=bias,\n",
    "            norm_layer=norm_layer,\n",
    "            softmax_scale=softmax_scale,\n",
    "            attn_dropout_p=attn_dropout_p,\n",
    "            var_attn_bias=var_attn_bias,\n",
    "            time_attn_bias=time_attn_bias,\n",
    "            var_qk_proj=var_qk_proj,\n",
    "            time_qk_proj=time_qk_proj,\n",
    "        )\n",
    "\n",
    "\n",
    "class MultiHeadAttention(GroupedQueryAttention):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        bias: bool = True,\n",
    "        norm_layer: Optional[type[nn.Module] | partial[nn.Module]] = nn.LayerNorm,\n",
    "        softmax_scale: Optional[float] = None,\n",
    "        attn_dropout_p: float = 0.0,\n",
    "        var_attn_bias: Optional[Callable[[], AttentionBias]] = None,\n",
    "        time_attn_bias: Optional[Callable[[], AttentionBias]] = None,\n",
    "        var_qk_proj: Optional[Callable[[], QueryKeyProjection]] = None,\n",
    "        time_qk_proj: Optional[Callable[[], QueryKeyProjection]] = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            dim=dim,\n",
    "            num_heads=num_heads,\n",
    "            num_groups=num_heads,\n",
    "            bias=bias,\n",
    "            norm_layer=norm_layer,\n",
    "            softmax_scale=softmax_scale,\n",
    "            attn_dropout_p=attn_dropout_p,\n",
    "            var_attn_bias=var_attn_bias,\n",
    "            time_attn_bias=time_attn_bias,\n",
    "            var_qk_proj=var_qk_proj,\n",
    "            time_qk_proj=time_qk_proj,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from jaxtyping import Float\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        hidden_dim: Optional[int] = None,\n",
    "        out_dim: Optional[int] = None,\n",
    "        activation: Callable[[torch.Tensor], torch.Tensor] = F.gelu,\n",
    "        bias: bool = True,\n",
    "        ffn_dropout_p: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_dim = hidden_dim or 4 * in_dim\n",
    "        out_dim = out_dim or in_dim\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.bias = bias\n",
    "        self.ffn_dropout_p = ffn_dropout_p\n",
    "\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim, bias=bias)\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim, bias=bias)\n",
    "        self.dropout1 = nn.Dropout(ffn_dropout_p)\n",
    "        self.dropout2 = nn.Dropout(ffn_dropout_p)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(\n",
    "        self, x: Float[torch.Tensor, \"... in_dim\"]\n",
    "    ) -> Float[torch.Tensor, \"... out_dim\"]:\n",
    "        x = self._in_proj(x)\n",
    "        return self.dropout2(self.fc2(self.dropout1(x)))\n",
    "\n",
    "    def _in_proj(\n",
    "        self, x: Float[torch.Tensor, \"... in_dim\"]\n",
    "    ) -> Float[torch.Tensor, \"... out_dim\"]:\n",
    "        return self.activation(self.fc1(x))\n",
    "\n",
    "\n",
    "class GatedLinearUnitFeedForward(FeedForward):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        hidden_dim: Optional[int] = None,\n",
    "        out_dim: Optional[int] = None,\n",
    "        activation: Callable[[torch.Tensor], torch.Tensor] = F.silu,\n",
    "        bias: bool = True,\n",
    "        ffn_dropout_p: float = 0.0,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            in_dim,\n",
    "            hidden_dim=hidden_dim or self.adjust_hidden_dim(4 * in_dim),\n",
    "            out_dim=out_dim,\n",
    "            activation=activation,\n",
    "            bias=bias,\n",
    "            ffn_dropout_p=ffn_dropout_p,\n",
    "        )\n",
    "        self.fc_gate = nn.Linear(self.in_dim, self.hidden_dim, bias=self.bias)\n",
    "\n",
    "    @staticmethod\n",
    "    def adjust_hidden_dim(dim):\n",
    "        return (int(dim * 2 / 3) + 7) // 8 * 8\n",
    "\n",
    "    def _in_proj(\n",
    "        self, x: Float[torch.Tensor, \"... in_dim\"]\n",
    "    ) -> Float[torch.Tensor, \"... out_dim\"]:\n",
    "        return self.activation(self.fc_gate(x)) * self.fc1(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "from functools import partial\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from jaxtyping import Bool, Float, Int\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        self_attn: GroupedQueryAttention,\n",
    "        ffn: FeedForward,\n",
    "        norm1: Optional[nn.Module],\n",
    "        norm2: Optional[nn.Module],\n",
    "        post_attn_dropout_p: float = 0.0,\n",
    "        pre_norm: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pre_norm = pre_norm\n",
    "        self.dropout_p = post_attn_dropout_p\n",
    "\n",
    "        self.self_attn = self_attn\n",
    "        self.ffn = ffn\n",
    "        self.norm1 = norm1 or nn.Identity()\n",
    "        self.norm2 = norm2 or nn.Identity()\n",
    "        self.dropout = nn.Dropout(post_attn_dropout_p)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Float[torch.Tensor, \"*batch time_len dim\"],\n",
    "        attn_mask: Optional[Bool[torch.Tensor, \"*batch time_len time_len\"]] = None,\n",
    "        var_id: Optional[Int[torch.Tensor, \"*batch time_len\"]] = None,\n",
    "        time_id: Optional[Int[torch.Tensor, \"*batch time_len\"]] = None,\n",
    "    ) -> Float[torch.Tensor, \"*batch time_len dim\"]:\n",
    "        if self.pre_norm:\n",
    "            x = x + self._sa_block(\n",
    "                self.norm1(x), attn_mask, var_id=var_id, time_id=time_id\n",
    "            )\n",
    "            x = x + self.ffn(self.norm2(x))\n",
    "        else:\n",
    "            x = self.norm1(\n",
    "                x + self._sa_block(x, attn_mask, var_id=var_id, time_id=time_id)\n",
    "            )\n",
    "            x = self.norm2(x + self.ffn(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _sa_block(\n",
    "        self,\n",
    "        x: Float[torch.Tensor, \"*batch time_len dim\"],\n",
    "        attn_mask: Optional[Bool[torch.Tensor, \"*batch time_len time_len\"]],\n",
    "        var_id: Optional[Int[torch.Tensor, \"*batch time_len\"]] = None,\n",
    "        time_id: Optional[Int[torch.Tensor, \"*batch time_len\"]] = None,\n",
    "    ) -> Float[torch.Tensor, \"*batch time_len dim\"]:\n",
    "        x = self.self_attn(\n",
    "            x,\n",
    "            x,\n",
    "            x,\n",
    "            attn_mask=attn_mask,\n",
    "            query_var_id=var_id,\n",
    "            kv_var_id=var_id,\n",
    "            query_time_id=time_id,\n",
    "            kv_time_id=time_id,\n",
    "        )\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        num_layers: int,\n",
    "        num_heads: Optional[int] = None,\n",
    "        num_groups: Optional[int] = None,\n",
    "        pre_norm: bool = True,\n",
    "        attn_dropout_p: float = 0.0,\n",
    "        dropout_p: float = 0.0,\n",
    "        norm_layer: Optional[Callable[[int], nn.Module]] = nn.LayerNorm,\n",
    "        activation: Callable[[torch.Tensor], torch.Tensor] = F.silu,\n",
    "        use_glu: bool = True,\n",
    "        use_qk_norm: bool = True,\n",
    "        var_attn_bias_layer: Optional[Callable[[int, int, int], AttentionBias]] = None,\n",
    "        time_attn_bias_layer: Optional[Callable[[int, int, int], AttentionBias]] = None,\n",
    "        var_qk_proj_layer: Optional[\n",
    "            Callable[[int, int, int], QueryKeyProjection]\n",
    "        ] = None,\n",
    "        time_qk_proj_layer: Optional[\n",
    "            Callable[[int, int, int], QueryKeyProjection]\n",
    "        ] = None,\n",
    "        shared_var_attn_bias: bool = False,\n",
    "        shared_time_attn_bias: bool = False,\n",
    "        shared_var_qk_proj: bool = False,\n",
    "        shared_time_qk_proj: bool = False,\n",
    "        d_ff: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        num_heads = num_heads or d_model // 64\n",
    "        num_groups = num_groups or num_heads  # defaults to mha\n",
    "\n",
    "        var_attn_bias = self.get_layer(\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            num_groups,\n",
    "            var_attn_bias_layer,\n",
    "            shared_var_attn_bias,\n",
    "        )\n",
    "        time_attn_bias = self.get_layer(\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            num_groups,\n",
    "            time_attn_bias_layer,\n",
    "            shared_time_attn_bias,\n",
    "        )\n",
    "        var_qk_proj = self.get_layer(\n",
    "            d_model, num_heads, num_groups, var_qk_proj_layer, shared_var_qk_proj\n",
    "        )\n",
    "        time_qk_proj = self.get_layer(\n",
    "            d_model, num_heads, num_groups, time_qk_proj_layer, shared_time_qk_proj\n",
    "        )\n",
    "\n",
    "        get_self_attn = partial(\n",
    "            GroupedQueryAttention,\n",
    "            dim=d_model,\n",
    "            num_heads=num_heads,\n",
    "            num_groups=num_groups,\n",
    "            bias=False,\n",
    "            norm_layer=norm_layer if use_qk_norm else None,\n",
    "            softmax_scale=None,\n",
    "            attn_dropout_p=attn_dropout_p,\n",
    "            var_attn_bias=var_attn_bias,\n",
    "            time_attn_bias=time_attn_bias,\n",
    "            var_qk_proj=var_qk_proj,\n",
    "            time_qk_proj=time_qk_proj,\n",
    "        )\n",
    "        get_ffn = partial(\n",
    "            GatedLinearUnitFeedForward if use_glu else FeedForward,\n",
    "            in_dim=d_model,\n",
    "            hidden_dim=d_ff,\n",
    "            out_dim=None,\n",
    "            activation=activation,\n",
    "            bias=False,\n",
    "            ffn_dropout_p=dropout_p,\n",
    "        )\n",
    "        get_encoder_layer_norm = partial(norm_layer, d_model)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerEncoderLayer(\n",
    "                    self_attn=get_self_attn(),\n",
    "                    ffn=get_ffn(),\n",
    "                    norm1=get_encoder_layer_norm(),\n",
    "                    norm2=get_encoder_layer_norm(),\n",
    "                    pre_norm=pre_norm,\n",
    "                    post_attn_dropout_p=dropout_p,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = norm_layer(d_model)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_layer(\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        num_groups: int,\n",
    "        layer: Callable,\n",
    "        shared_layer: bool,\n",
    "    ) -> Optional[Callable[[], nn.Module]]:\n",
    "        if layer is None:\n",
    "            return None\n",
    "        if shared_layer:\n",
    "            module = layer(dim=dim, num_heads=num_heads, num_groups=num_groups)\n",
    "            return lambda: module\n",
    "        return partial(layer, dim=dim, num_heads=num_heads, num_groups=num_groups)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Float[torch.Tensor, \"*batch time_len dim\"],\n",
    "        attn_mask: Optional[Bool[torch.Tensor, \"*batch time_len time_len\"]] = None,\n",
    "        var_id: Optional[Int[torch.Tensor, \"*batch time_len\"]] = None,\n",
    "        time_id: Optional[Int[torch.Tensor, \"*batch time_len\"]] = None,\n",
    "    ) -> Float[torch.Tensor, \"*batch time_len dim\"]:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attn_mask, var_id=var_id, time_id=time_id)\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'uni2ts'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Distribution\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pytree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tree_map\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01muni2ts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mask_fill, packed_attention_mask\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01muni2ts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribution\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistributionOutput\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01muni2ts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnorm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RMSNorm\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'uni2ts'"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "#from hydra.utils import instantiate\n",
    "from jaxtyping import Bool, Float, Int\n",
    "from torch import nn\n",
    "from torch.distributions import Distribution\n",
    "from torch.utils._pytree import tree_map\n",
    "\n",
    "from uni2ts.common.torch_util import mask_fill, packed_attention_mask\n",
    "from uni2ts.distribution import DistributionOutput\n",
    "from uni2ts.module.norm import RMSNorm\n",
    "from uni2ts.module.packed_scaler import PackedNOPScaler, PackedStdScaler\n",
    "from uni2ts.module.position import (\n",
    "    BinaryAttentionBias,\n",
    "    QueryKeyProjection,\n",
    "    RotaryProjection,\n",
    ")\n",
    "from uni2ts.module.transformer import TransformerEncoder\n",
    "from uni2ts.module.ts_embed import MultiInSizeLinear\n",
    "\n",
    "\n",
    "def encode_distr_output(\n",
    "    distr_output: DistributionOutput,\n",
    ") -> dict[str, str | float | int]:\n",
    "    \"\"\"Serialization function for DistributionOutput\"\"\"\n",
    "\n",
    "    def _encode(val):\n",
    "        if not isinstance(val, DistributionOutput):\n",
    "            return val\n",
    "\n",
    "        return {\n",
    "            \"_target_\": f\"{val.__class__.__module__}.{val.__class__.__name__}\",\n",
    "            **tree_map(_encode, val.__dict__),\n",
    "        }\n",
    "\n",
    "    return _encode(distr_output)\n",
    "\n",
    "\n",
    "#def decode_distr_output(config: dict[str, str | float | int]) -> DistributionOutput:\n",
    " #   \"\"\"Deserialization function for DistributionOutput\"\"\"\n",
    "  #  return instantiate(config, _convert_=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PyTorchModelHubMixin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMoiraiModule\u001b[39;00m(\n\u001b[0;32m      2\u001b[0m     nn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mPyTorchModelHubMixin\u001b[49m,\n\u001b[0;32m      4\u001b[0m     coders\u001b[38;5;241m=\u001b[39m{DistributionOutput: (encode_distr_output, decode_distr_output)},\n\u001b[0;32m      5\u001b[0m ):\n\u001b[0;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m    Contains components of Moirai, to ensure implementation is identical across models.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m    Subclasses huggingface_hub.PyTorchModelHubMixin to support loading from HuggingFace Hub.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     13\u001b[0m         distr_output: DistributionOutput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m         scaling: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     21\u001b[0m     ):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PyTorchModelHubMixin' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class MoiraiModule(\n",
    "    nn.Module,\n",
    "    PyTorchModelHubMixin,\n",
    "    coders={DistributionOutput: (encode_distr_output, decode_distr_output)},\n",
    "):\n",
    "    \"\"\"\n",
    "    Contains components of Moirai, to ensure implementation is identical across models.\n",
    "    Subclasses huggingface_hub.PyTorchModelHubMixin to support loading from HuggingFace Hub.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        distr_output: DistributionOutput,\n",
    "        d_model: int,\n",
    "        num_layers: int,\n",
    "        patch_sizes: tuple[int, ...],  # tuple[int, ...] | list[int]\n",
    "        max_seq_len: int,\n",
    "        attn_dropout_p: float,\n",
    "        dropout_p: float,\n",
    "        scaling: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param distr_output: distribution output object\n",
    "        :param d_model: model hidden dimensions\n",
    "        :param num_layers: number of transformer layers\n",
    "        :param patch_sizes: sequence of patch sizes\n",
    "        :param max_seq_len: maximum sequence length for inputs\n",
    "        :param attn_dropout_p: dropout probability for attention layers\n",
    "        :param dropout_p: dropout probability for all other layers\n",
    "        :param scaling: whether to apply scaling (standardization)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.patch_sizes = patch_sizes\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.scaling = scaling\n",
    "\n",
    "        self.mask_encoding = nn.Embedding(num_embeddings=1, embedding_dim=d_model)\n",
    "        self.scaler = PackedStdScaler() if scaling else PackedNOPScaler()\n",
    "        self.in_proj = MultiInSizeLinear(\n",
    "            in_features_ls=patch_sizes,\n",
    "            out_features=d_model,\n",
    "        )\n",
    "        self.encoder = TransformerEncoder(\n",
    "            d_model,\n",
    "            num_layers,\n",
    "            num_heads=None,\n",
    "            pre_norm=True,\n",
    "            attn_dropout_p=attn_dropout_p,\n",
    "            dropout_p=dropout_p,\n",
    "            norm_layer=RMSNorm,\n",
    "            activation=F.silu,\n",
    "            use_glu=True,\n",
    "            use_qk_norm=True,\n",
    "            var_attn_bias_layer=partial(BinaryAttentionBias),\n",
    "            time_qk_proj_layer=partial(\n",
    "                QueryKeyProjection,\n",
    "                proj_layer=RotaryProjection,\n",
    "                kwargs=dict(max_len=max_seq_len),\n",
    "                partial_factor=(0.0, 0.5),\n",
    "            ),\n",
    "            shared_var_attn_bias=False,\n",
    "            shared_time_qk_proj=True,\n",
    "            d_ff=None,\n",
    "        )\n",
    "        self.distr_output = distr_output\n",
    "        self.param_proj = self.distr_output.get_param_proj(d_model, patch_sizes)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        target: Float[torch.Tensor, \"*batch seq_len max_patch\"],\n",
    "        observed_mask: Bool[torch.Tensor, \"*batch seq_len max_patch\"],\n",
    "        sample_id: Int[torch.Tensor, \"*batch seq_len\"],\n",
    "        time_id: Int[torch.Tensor, \"*batch seq_len\"],\n",
    "        variate_id: Int[torch.Tensor, \"*batch seq_len\"],\n",
    "        prediction_mask: Bool[torch.Tensor, \"*batch seq_len\"],\n",
    "        patch_size: Int[torch.Tensor, \"*batch seq_len\"],\n",
    "    ) -> Distribution:\n",
    "        \"\"\"\n",
    "        Defines the forward pass of MoiraiModule.\n",
    "        This method expects processed inputs.\n",
    "\n",
    "        1. Apply scaling to observations\n",
    "        2. Project from observations to representations\n",
    "        3. Replace prediction window with learnable mask\n",
    "        4. Apply transformer layers\n",
    "        5. Project from representations to distribution parameters\n",
    "        6. Return distribution object\n",
    "\n",
    "        :param target: input data\n",
    "        :param observed_mask: binary mask for missing values, 1 if observed, 0 otherwise\n",
    "        :param sample_id: indices indicating the sample index (for packing)\n",
    "        :param time_id: indices indicating the time index\n",
    "        :param variate_id: indices indicating the variate index\n",
    "        :param prediction_mask: binary mask for prediction horizon, 1 if part of the horizon, 0 otherwise\n",
    "        :param patch_size: patch size for each token\n",
    "        :return: predictive distribution\n",
    "        \"\"\"\n",
    "        loc, scale = self.scaler(\n",
    "            target,\n",
    "            observed_mask * ~prediction_mask.unsqueeze(-1),\n",
    "            sample_id,\n",
    "            variate_id,\n",
    "        )\n",
    "        scaled_target = (target - loc) / scale\n",
    "        reprs = self.in_proj(scaled_target, patch_size)\n",
    "        masked_reprs = mask_fill(reprs, prediction_mask, self.mask_encoding.weight)\n",
    "        reprs = self.encoder(\n",
    "            masked_reprs,\n",
    "            packed_attention_mask(sample_id),\n",
    "            time_id=time_id,\n",
    "            var_id=variate_id,\n",
    "        )\n",
    "        distr_param = self.param_proj(reprs, patch_size)\n",
    "        distr = self.distr_output.distribution(distr_param, loc=loc, scale=scale)\n",
    "        return distr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the data generation function\n",
    "def generate_data(num_points):\n",
    "    x = np.linspace(0, 2 * math.pi*20, num_points)\n",
    "    sine_data = np.sin(x)\n",
    "    cosine_data = np.cos(x)\n",
    "    return np.array(list(zip(sine_data, cosine_data)))\n",
    "\n",
    "# Define a simple Time Series Dataset\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, input_len, output_len):\n",
    "        self.data = data\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.input_len - self.output_len + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx+self.input_len]\n",
    "        y = self.data[idx+self.input_len:idx+self.input_len+self.output_len]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Parameters for dataset\n",
    "input_len = 20\n",
    "output_len = 20\n",
    "num_points = 1000\n",
    "\n",
    "# Generate data\n",
    "data = generate_data(num_points)\n",
    "\n",
    "# Create dataset\n",
    "dataset = TimeSeriesDataset(data, input_len, output_len)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# Split dataset\n",
    "train_dataset, test_dataset = Subset(dataset, range(train_size)), Subset(dataset, range(train_size, len(dataset)))\n",
    "\n",
    "\n",
    "plt.plot(data)\n",
    "plt.axvline(x=train_size, color='gray', linestyle='--')  # Vertical line separating input and output\n",
    "\n",
    "\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Assuming 'Model' is already defined and imported\n",
    "class Args() :\n",
    "    def __init__(self):\n",
    "        self.task_name= 'forecast'\n",
    "        self.ckpt_path= ''\n",
    "        self.patch_len= 10\n",
    "        self.d_model= 64\n",
    "        self.d_ff= 256\n",
    "        self.e_layers= 3\n",
    "        self.n_heads= 4\n",
    "        self.dropout= 0.1\n",
    "        self.output_attention= False\n",
    "        self.factor= False\n",
    "        self.activation= False\n",
    "\n",
    "args = Args()\n",
    "# Instantiate model\n",
    "model = Model(args)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(10):  # Number of epochs\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs,None,None,None)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Average Loss: {total_loss/len(train_loader)}')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs,None,None,None)\n",
    "        loss = criterion(outputs, targets)\n",
    "        print(f'Test Loss: {loss.item()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
